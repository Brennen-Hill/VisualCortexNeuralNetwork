{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7-uI277OdfP",
        "outputId": "a569ea1a-a825-483c-c3b2-76f8cbb91105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "BrainInspiredVisualNetwork(\n",
            "  (v1): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (thick_stripe): Sequential(\n",
            "    (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (interstripe): Sequential(\n",
            "    (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (thin_stripe): Sequential(\n",
            "    (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (mt): Sequential(\n",
            "    (0): Conv2d(48, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (vip): Sequential(\n",
            "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (mst): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (v4): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (lip): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (pit): Sequential(\n",
            "    (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (cit): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer_7a): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (ait): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): AdaptiveAvgPool2d(output_size=1)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=32, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch: 0, Batch: 0, Loss: 2.2966, Acc: 7.81%\n",
            "Epoch: 0, Batch: 100, Loss: 0.7078, Acc: 70.88%\n",
            "Epoch: 0, Batch: 200, Loss: 0.2226, Acc: 83.15%\n",
            "Epoch: 0, Batch: 300, Loss: 0.1637, Acc: 87.69%\n",
            "Epoch: 0, Batch: 400, Loss: 0.0813, Acc: 90.17%\n",
            "Epoch: 0, Batch: 500, Loss: 0.0832, Acc: 91.69%\n",
            "Epoch: 0, Batch: 600, Loss: 0.1493, Acc: 92.72%\n",
            "Epoch: 0, Batch: 700, Loss: 0.0963, Acc: 93.48%\n",
            "Epoch: 0, Batch: 800, Loss: 0.1489, Acc: 94.07%\n",
            "Epoch: 0, Batch: 900, Loss: 0.1263, Acc: 94.53%\n",
            "Test set: Average loss: 0.0950, Accuracy: 97.30%\n",
            "Epoch 1/10, Train Loss: 0.2650, Train Acc: 94.67%, Test Loss: 0.0950, Test Acc: 97.30%\n",
            "Epoch: 1, Batch: 0, Loss: 0.0768, Acc: 98.44%\n",
            "Epoch: 1, Batch: 100, Loss: 0.0313, Acc: 98.53%\n",
            "Epoch: 1, Batch: 200, Loss: 0.0929, Acc: 98.51%\n",
            "Epoch: 1, Batch: 300, Loss: 0.0315, Acc: 98.53%\n",
            "Epoch: 1, Batch: 400, Loss: 0.1096, Acc: 98.43%\n",
            "Epoch: 1, Batch: 500, Loss: 0.0186, Acc: 98.51%\n",
            "Epoch: 1, Batch: 600, Loss: 0.0316, Acc: 98.50%\n",
            "Epoch: 1, Batch: 700, Loss: 0.0551, Acc: 98.53%\n",
            "Epoch: 1, Batch: 800, Loss: 0.0328, Acc: 98.58%\n",
            "Epoch: 1, Batch: 900, Loss: 0.0753, Acc: 98.61%\n",
            "Test set: Average loss: 0.0943, Accuracy: 97.14%\n",
            "Epoch 2/10, Train Loss: 0.0537, Train Acc: 98.60%, Test Loss: 0.0943, Test Acc: 97.14%\n",
            "Epoch: 2, Batch: 0, Loss: 0.0176, Acc: 100.00%\n",
            "Epoch: 2, Batch: 100, Loss: 0.0123, Acc: 98.98%\n",
            "Epoch: 2, Batch: 200, Loss: 0.0434, Acc: 98.87%\n",
            "Epoch: 2, Batch: 300, Loss: 0.0484, Acc: 98.83%\n",
            "Epoch: 2, Batch: 400, Loss: 0.0109, Acc: 98.93%\n",
            "Epoch: 2, Batch: 500, Loss: 0.0470, Acc: 98.91%\n",
            "Epoch: 2, Batch: 600, Loss: 0.0306, Acc: 98.89%\n",
            "Epoch: 2, Batch: 700, Loss: 0.0367, Acc: 98.90%\n",
            "Epoch: 2, Batch: 800, Loss: 0.0027, Acc: 98.89%\n",
            "Epoch: 2, Batch: 900, Loss: 0.0182, Acc: 98.89%\n",
            "Test set: Average loss: 0.0237, Accuracy: 99.22%\n",
            "Epoch 3/10, Train Loss: 0.0386, Train Acc: 98.90%, Test Loss: 0.0237, Test Acc: 99.22%\n",
            "Epoch: 3, Batch: 0, Loss: 0.0523, Acc: 98.44%\n",
            "Epoch: 3, Batch: 100, Loss: 0.0066, Acc: 99.29%\n",
            "Epoch: 3, Batch: 200, Loss: 0.0092, Acc: 99.11%\n",
            "Epoch: 3, Batch: 300, Loss: 0.0068, Acc: 99.08%\n",
            "Epoch: 3, Batch: 400, Loss: 0.0856, Acc: 99.06%\n",
            "Epoch: 3, Batch: 500, Loss: 0.0131, Acc: 99.08%\n",
            "Epoch: 3, Batch: 600, Loss: 0.0073, Acc: 99.07%\n",
            "Epoch: 3, Batch: 700, Loss: 0.0729, Acc: 99.06%\n",
            "Epoch: 3, Batch: 800, Loss: 0.0067, Acc: 99.09%\n",
            "Epoch: 3, Batch: 900, Loss: 0.0321, Acc: 99.10%\n",
            "Test set: Average loss: 0.0526, Accuracy: 98.45%\n",
            "Epoch 4/10, Train Loss: 0.0310, Train Acc: 99.09%, Test Loss: 0.0526, Test Acc: 98.45%\n",
            "Epoch: 4, Batch: 0, Loss: 0.0226, Acc: 100.00%\n",
            "Epoch: 4, Batch: 100, Loss: 0.0065, Acc: 99.10%\n",
            "Epoch: 4, Batch: 200, Loss: 0.0134, Acc: 99.15%\n",
            "Epoch: 4, Batch: 300, Loss: 0.0086, Acc: 99.18%\n",
            "Epoch: 4, Batch: 400, Loss: 0.0042, Acc: 99.13%\n",
            "Epoch: 4, Batch: 500, Loss: 0.0128, Acc: 99.13%\n",
            "Epoch: 4, Batch: 600, Loss: 0.0049, Acc: 99.11%\n",
            "Epoch: 4, Batch: 700, Loss: 0.0102, Acc: 99.10%\n",
            "Epoch: 4, Batch: 800, Loss: 0.0047, Acc: 99.08%\n",
            "Epoch: 4, Batch: 900, Loss: 0.0263, Acc: 99.09%\n",
            "Test set: Average loss: 0.0259, Accuracy: 99.12%\n",
            "Epoch 5/10, Train Loss: 0.0290, Train Acc: 99.10%, Test Loss: 0.0259, Test Acc: 99.12%\n",
            "Epoch: 5, Batch: 0, Loss: 0.0333, Acc: 98.44%\n",
            "Epoch: 5, Batch: 100, Loss: 0.0138, Acc: 99.32%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class BrainInspiredVisualNetwork(nn.Module):\n",
        "    def __init__(self, input_channels=1, base_features=32):\n",
        "        super(BrainInspiredVisualNetwork, self).__init__()\n",
        "\n",
        "        # V1 (Primary Visual Cortex) layer\n",
        "        self.v1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, base_features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features),\n",
        "            nn.Conv2d(base_features, base_features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features)\n",
        "        )\n",
        "\n",
        "        # Thick Stripe layer\n",
        "        self.thick_stripe = nn.Sequential(\n",
        "            nn.Conv2d(base_features, base_features//2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features//2)\n",
        "        )\n",
        "\n",
        "        # Interstripe layer\n",
        "        self.interstripe = nn.Sequential(\n",
        "            nn.Conv2d(base_features, base_features//2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features//2)\n",
        "        )\n",
        "\n",
        "        # Thin Stripe layer\n",
        "        self.thin_stripe = nn.Sequential(\n",
        "            nn.Conv2d(base_features, base_features//2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features//2)\n",
        "        )\n",
        "\n",
        "        # MT layer (Middle Temporal)\n",
        "        self.mt = nn.Sequential(\n",
        "            nn.Conv2d(base_features + base_features//2, base_features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features)\n",
        "        )\n",
        "\n",
        "        # VIP layer (Ventral Intraparietal)\n",
        "        self.vip = nn.Sequential(\n",
        "            nn.Conv2d(base_features, base_features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features)\n",
        "        )\n",
        "\n",
        "        # MST layer (Medial Superior Temporal)\n",
        "        self.mst = nn.Sequential(\n",
        "            nn.Conv2d(base_features*2, base_features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features)\n",
        "        )\n",
        "\n",
        "        # V4 layer\n",
        "        self.v4 = nn.Sequential(\n",
        "            nn.Conv2d(base_features + base_features//2*2, base_features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features)\n",
        "        )\n",
        "\n",
        "        # LIP layer (Lateral Intraparietal)\n",
        "        self.lip = nn.Sequential(\n",
        "            nn.Conv2d(base_features*2, base_features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features)\n",
        "        )\n",
        "\n",
        "        # PIT layer (Posterior Inferotemporal)\n",
        "        self.pit = nn.Sequential(\n",
        "            nn.Conv2d(base_features*3, base_features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features)\n",
        "        )\n",
        "\n",
        "        # CIT layer (Central Inferotemporal)\n",
        "        self.cit = nn.Sequential(\n",
        "            nn.Conv2d(base_features*2, base_features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features)\n",
        "        )\n",
        "\n",
        "        # 7A layer\n",
        "        self.layer_7a = nn.Sequential(\n",
        "            nn.Conv2d(base_features*2, base_features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features)\n",
        "        )\n",
        "\n",
        "        # AIT layer (Anterior Inferotemporal)\n",
        "        self.ait = nn.Sequential(\n",
        "            nn.Conv2d(base_features*2, base_features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(base_features),\n",
        "            nn.AdaptiveAvgPool2d(1)  # Global average pooling\n",
        "        )\n",
        "\n",
        "        # Classifier (final layer for MNIST)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(base_features, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process through V1 (input to V1)\n",
        "        v1_output = self.v1(x)\n",
        "\n",
        "        # Process V1 outputs\n",
        "        thick_stripe_output = self.thick_stripe(v1_output)\n",
        "        interstripe_output = self.interstripe(v1_output)\n",
        "        thin_stripe_output = self.thin_stripe(v1_output)\n",
        "\n",
        "        # MT layer (thick_stripe, v1)\n",
        "        mt_input = torch.cat([v1_output, thick_stripe_output], dim=1)\n",
        "        mt_output = self.mt(mt_input)\n",
        "\n",
        "        # VIP layer (MT)\n",
        "        vip_output = self.vip(mt_output)\n",
        "\n",
        "        # MST layer (MT, VIP)\n",
        "        mst_input = torch.cat([mt_output, vip_output], dim=1)\n",
        "        mst_output = self.mst(mst_input)\n",
        "\n",
        "        # V4 layer (MT, interstripe, thin_stripe)\n",
        "        v4_input = torch.cat([mt_output, interstripe_output, thin_stripe_output], dim=1)\n",
        "        v4_output = self.v4(v4_input)\n",
        "\n",
        "        # LIP layer (MST)\n",
        "        lip_input = torch.cat([mst_output, v1_output], dim=1)  # Added V1 for better feature propagation\n",
        "        lip_output = self.lip(lip_input)\n",
        "\n",
        "        # PIT layer (V4, MST, LIP)\n",
        "        pit_input = torch.cat([v4_output, mst_output, lip_output], dim=1)\n",
        "        pit_output = self.pit(pit_input)\n",
        "\n",
        "        # CIT layer (PIT, V4)\n",
        "        cit_input = torch.cat([pit_output, v4_output], dim=1)\n",
        "        cit_output = self.cit(cit_input)\n",
        "\n",
        "        # 7A layer (LIP, MST)\n",
        "        layer_7a_input = torch.cat([lip_output, mst_output], dim=1)\n",
        "        layer_7a_output = self.layer_7a(layer_7a_input)\n",
        "\n",
        "        # AIT layer (CIT, 7A)\n",
        "        ait_input = torch.cat([cit_output, layer_7a_output], dim=1)\n",
        "        ait_output = self.ait(ait_input)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.classifier(ait_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train(model, train_loader, optimizer, criterion, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
        "\n",
        "    return running_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "\n",
        "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "    return test_loss, accuracy\n",
        "\n",
        "def main():\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Hyperparameters\n",
        "    batch_size = 64\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    model = BrainInspiredVisualNetwork(input_channels=1, base_features=32).to(device)\n",
        "    print(model)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training history\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, epoch)\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "    # Plot training and validation loss and accuracy\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(test_losses, label='Test Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss over epochs')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(test_accuracies, label='Test Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy over epochs')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_results.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), 'brain_inspired_model.pth')\n",
        "    print(\"Model saved successfully!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}