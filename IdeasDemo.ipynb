{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Predictive Coding with Top‑Down Feedback Concept:**\n",
        "\n",
        "\n",
        "Higher layers generate predictions of lower‑level activations.\n",
        "\n",
        "\n",
        "Compare these predictions with the actual bottom‑up activations to compute an error signal.\n",
        "\n",
        "\n",
        "Use the error to refine the lower‑level representation.\n",
        "\n",
        "\n",
        "**Implementation Outline:**\n",
        "\n",
        "\n",
        "Create a top‑down convolution (or deconvolution) module to “predict” lower‑level activations from higher layers.\n",
        "\n",
        "\n",
        "Compute the difference (error) between the actual lower‑level output and the prediction.\n",
        "\n",
        "\n",
        "Feed the error back (e.g. add it to the lower‑level activation) and possibly iterate to refine the representation."
      ],
      "metadata": {
        "id": "V7F5kVeF3BSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Predictive Coding Block ---\n",
        "class PredictiveCodingBlock(nn.Module):\n",
        "    def __init__(self, bottom_up_conv, top_down_conv):\n",
        "        \"\"\"\n",
        "        bottom_up_conv: convolution used to extract bottom-up features\n",
        "        top_down_conv: convolution that generates a top-down prediction\n",
        "        \"\"\"\n",
        "        super(PredictiveCodingBlock, self).__init__()\n",
        "        self.bottom_up_conv = bottom_up_conv\n",
        "        self.top_down_conv = top_down_conv\n",
        "\n",
        "    def forward(self, lower_input, higher_prediction):\n",
        "        # Bottom-up feature extraction from lower layer input.\n",
        "        bottom_up = self.bottom_up_conv(lower_input)\n",
        "        # Top-down prediction from a higher-level representation.\n",
        "        top_down = self.top_down_conv(higher_prediction)\n",
        "        # Compute prediction error; ReLU ensures a non-negative error.\n",
        "        error = F.relu(bottom_up - top_down)\n",
        "        return error\n",
        "\n",
        "# --- Neural Network with Predictive Coding ---\n",
        "class NeuralNetworkPredictiveCoding(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_dim, output_dim, kernel_size=3, padding_size=1):\n",
        "        super(NeuralNetworkPredictiveCoding, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        # Define multipliers (simulating different brain areas)\n",
        "        V1_p = 10; Thick_stripe_p = 1; MT_p = 1; VIP_p = 0.4; MST_p = 0.5\n",
        "        Interstripe_p = 5; Thin_stripe_p = 1; LIP_p = 1; V4_p = 4\n",
        "        PIT_p = 2.5; CIT_p = 3.5; SevenA_p = 3.5; AIT_p = 4.5\n",
        "\n",
        "        # Convolutional layers for each brain area stage.\n",
        "        self.V1 = nn.Conv2d(input_channels, int(hidden_dim * V1_p), kernel_size, padding=padding_size)\n",
        "        self.ThickStripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Thick_stripe_p), kernel_size, padding=padding_size)\n",
        "        self.MT = nn.Conv2d(int(hidden_dim * (V1_p + Thick_stripe_p)), int(hidden_dim * MT_p), kernel_size, padding=padding_size)\n",
        "        self.VIP = nn.Conv2d(int(hidden_dim * MT_p), int(hidden_dim * VIP_p), kernel_size, padding=padding_size)\n",
        "        self.MST = nn.Conv2d(int(hidden_dim * (MT_p + VIP_p)), int(hidden_dim * MST_p), kernel_size, padding=padding_size)\n",
        "        self.Interstripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Interstripe_p), kernel_size, padding=padding_size)\n",
        "        self.ThinStripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Thin_stripe_p), kernel_size, padding=padding_size)\n",
        "        self.LIP = nn.Conv2d(int(hidden_dim * MST_p), int(hidden_dim * LIP_p), kernel_size, padding=padding_size)\n",
        "        self.V4 = nn.Conv2d(int(hidden_dim * (MT_p + Interstripe_p + Thin_stripe_p)), int(hidden_dim * V4_p), kernel_size, padding=padding_size)\n",
        "        self.PIT = nn.Conv2d(int(hidden_dim * (V4_p + MST_p + LIP_p)), int(hidden_dim * PIT_p), kernel_size, padding=padding_size)\n",
        "        self.CIT = nn.Conv2d(int(hidden_dim * (PIT_p + V4_p)), int(hidden_dim * CIT_p), kernel_size, padding=padding_size)\n",
        "        self.SevenA = nn.Conv2d(int(hidden_dim * (MST_p + LIP_p)), int(hidden_dim * SevenA_p), kernel_size, padding=padding_size)\n",
        "        self.AIT = nn.Conv2d(int(hidden_dim * (CIT_p + SevenA_p)), output_dim, kernel_size, padding=padding_size)\n",
        "\n",
        "        # Global average pooling to collapse spatial dimensions.\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Predictive coding: use a top-down projection from AIT to predict V1.\n",
        "        self.pred_V1 = PredictiveCodingBlock(\n",
        "            bottom_up_conv=self.V1,\n",
        "            top_down_conv=nn.Conv2d(output_dim, int(hidden_dim * V1_p), kernel_size, padding=padding_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through V1 and subsequent layers.\n",
        "        V1 = self.relu(self.V1(x))\n",
        "        ThickStripe = self.relu(self.ThickStripe(V1))\n",
        "        MT_input = torch.cat((V1, ThickStripe), dim=1)\n",
        "        MT = self.relu(self.MT(MT_input))\n",
        "        VIP = self.relu(self.VIP(MT))\n",
        "        MST_input = torch.cat((MT, VIP), dim=1)\n",
        "        MST = self.relu(self.MST(MST_input))\n",
        "        Interstripe = self.relu(self.Interstripe(V1))\n",
        "        ThinStripe = self.relu(self.ThinStripe(V1))\n",
        "        LIP = self.relu(self.LIP(MST))\n",
        "        V4_input = torch.cat((MT, Interstripe, ThinStripe), dim=1)\n",
        "        V4 = self.relu(self.V4(V4_input))\n",
        "        PIT_input = torch.cat((V4, MST, LIP), dim=1)\n",
        "        PIT = self.relu(self.PIT(PIT_input))\n",
        "        CIT_input = torch.cat((PIT, V4), dim=1)\n",
        "        CIT = self.relu(self.CIT(CIT_input))\n",
        "        SevenA = self.relu(self.SevenA(torch.cat((MST, LIP), dim=1)))\n",
        "        AIT_input = torch.cat((CIT, SevenA), dim=1)\n",
        "        AIT = self.AIT(AIT_input)\n",
        "\n",
        "        # Compute prediction error from AIT feedback to V1.\n",
        "        pred_error = self.pred_V1(x, AIT)\n",
        "        # Refine V1 representation by adding the error signal.\n",
        "        V1_refined = V1 + pred_error\n",
        "\n",
        "        # Final global pooling (here we classify based on AIT output).\n",
        "        out = self.pool(AIT)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "\n",
        "# --- Testing the Predictive Coding Network ---\n",
        "if __name__ == '__main__':\n",
        "    net = NeuralNetworkPredictiveCoding(input_channels=3, hidden_dim=16, output_dim=10)\n",
        "    dummy_input = torch.randn(1, 3, 64, 64)\n",
        "    output = net(dummy_input)\n",
        "    print(\"Predictive Coding Network Output Shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9zvjHAa0DA3",
        "outputId": "5d94c36b-f902-4011-f62b-3600f4e9eff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictive Coding Network Output Shape: torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feedback Loops via Recurrent Units Concept:**\n",
        "\n",
        "Recurrent connections allow the network to “unfold” its computations over time.\n",
        "\n",
        "This can help to iteratively refine representations.\n",
        "\n",
        "**Implementation Outline:**\n",
        "\n",
        "Wrap a convolution (or block) inside a recurrent structure (e.g. a ConvRNN or simply use a loop that re‑applies the same layer with a residual connection).\n",
        "\n",
        "Unroll the recurrence for a fixed number of iterations, updating a hidden state."
      ],
      "metadata": {
        "id": "aKjmd6k-3YNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Recurrent Block for Feedback Loops ---\n",
        "class RecurrentBlock(nn.Module):\n",
        "    def __init__(self, conv_layer, num_iterations=3):\n",
        "        \"\"\"\n",
        "        Wraps a convolutional layer in a recurrent loop.\n",
        "        num_iterations: number of recurrent iterations.\n",
        "        \"\"\"\n",
        "        super(RecurrentBlock, self).__init__()\n",
        "        self.conv_layer = conv_layer\n",
        "        self.num_iterations = num_iterations\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        # Iteratively refine the representation with a residual connection.\n",
        "        for _ in range(self.num_iterations):\n",
        "            h = F.relu(self.conv_layer(h) + x)\n",
        "        return h\n",
        "\n",
        "# --- Neural Network with Recurrent Feedback ---\n",
        "class NeuralNetworkRecurrentFeedback(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_dim, output_dim, kernel_size=3, padding_size=1):\n",
        "        super(NeuralNetworkRecurrentFeedback, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        # Multipliers for different processing stages.\n",
        "        V1_p = 10; Thick_stripe_p = 1; MT_p = 1; VIP_p = 0.4; MST_p = 0.5\n",
        "        Interstripe_p = 5; Thin_stripe_p = 1; LIP_p = 1; V4_p = 4\n",
        "        PIT_p = 2.5; CIT_p = 3.5; SevenA_p = 3.5; AIT_p = 4.5\n",
        "\n",
        "        # Define convolutional layers.\n",
        "        self.V1 = nn.Conv2d(input_channels, int(hidden_dim * V1_p), kernel_size, padding=padding_size)\n",
        "        self.ThickStripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Thick_stripe_p), kernel_size, padding=padding_size)\n",
        "        self.MT = nn.Conv2d(int(hidden_dim * (V1_p + Thick_stripe_p)), int(hidden_dim * MT_p), kernel_size, padding=padding_size)\n",
        "        self.VIP = nn.Conv2d(int(hidden_dim * MT_p), int(hidden_dim * VIP_p), kernel_size, padding=padding_size)\n",
        "        self.MST = nn.Conv2d(int(hidden_dim * (MT_p + VIP_p)), int(hidden_dim * MST_p), kernel_size, padding=padding_size)\n",
        "        self.Interstripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Interstripe_p), kernel_size, padding=padding_size)\n",
        "        self.ThinStripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Thin_stripe_p), kernel_size, padding=padding_size)\n",
        "        self.LIP = nn.Conv2d(int(hidden_dim * MST_p), int(hidden_dim * LIP_p), kernel_size, padding=padding_size)\n",
        "        self.V4 = nn.Conv2d(int(hidden_dim * (MT_p + Interstripe_p + Thin_stripe_p)), int(hidden_dim * V4_p), kernel_size, padding=padding_size)\n",
        "        self.PIT = nn.Conv2d(int(hidden_dim * (V4_p + MST_p + LIP_p)), int(hidden_dim * PIT_p), kernel_size, padding=padding_size)\n",
        "        self.CIT = nn.Conv2d(int(hidden_dim * (PIT_p + V4_p)), int(hidden_dim * CIT_p), kernel_size, padding=padding_size)\n",
        "        self.SevenA = nn.Conv2d(int(hidden_dim * (MST_p + LIP_p)), int(hidden_dim * SevenA_p), kernel_size, padding=padding_size)\n",
        "        self.AIT = nn.Conv2d(int(hidden_dim * (CIT_p + SevenA_p)), output_dim, kernel_size, padding=padding_size)\n",
        "\n",
        "        # Global pooling layer.\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Wrap MT and MST layers with recurrent feedback blocks.\n",
        "        self.recur_MT = RecurrentBlock(self.MT, num_iterations=3)\n",
        "        self.recur_MST = RecurrentBlock(self.MST, num_iterations=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        V1 = self.relu(self.V1(x))\n",
        "        ThickStripe = self.relu(self.ThickStripe(V1))\n",
        "        MT_input = torch.cat((V1, ThickStripe), dim=1)\n",
        "        MT = self.relu(self.MT(MT_input))\n",
        "        # Apply recurrent feedback on MT.\n",
        "        MT_refined = self.recur_MT(MT)\n",
        "\n",
        "        VIP = self.relu(self.VIP(MT_refined))\n",
        "        MST_input = torch.cat((MT_refined, VIP), dim=1)\n",
        "        MST = self.relu(self.MST(MST_input))\n",
        "        # Apply recurrent feedback on MST.\n",
        "        MST_refined = self.recur_MST(MST)\n",
        "\n",
        "        Interstripe = self.relu(self.Interstripe(V1))\n",
        "        ThinStripe = self.relu(self.ThinStripe(V1))\n",
        "        LIP = self.relu(self.LIP(MST_refined))\n",
        "        V4_input = torch.cat((MT_refined, Interstripe, ThinStripe), dim=1)\n",
        "        V4 = self.relu(self.V4(V4_input))\n",
        "        PIT_input = torch.cat((V4, MST_refined, LIP), dim=1)\n",
        "        PIT = self.relu(self.PIT(PIT_input))\n",
        "        CIT_input = torch.cat((PIT, V4), dim=1)\n",
        "        CIT = self.relu(self.CIT(CIT_input))\n",
        "        SevenA = self.relu(self.SevenA(torch.cat((MST_refined, LIP), dim=1)))\n",
        "        AIT_input = torch.cat((CIT, SevenA), dim=1)\n",
        "        AIT = self.AIT(AIT_input)\n",
        "\n",
        "        out = self.pool(AIT)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "\n",
        "# --- Testing the Recurrent Feedback Network ---\n",
        "if __name__ == '__main__':\n",
        "    net = NeuralNetworkRecurrentFeedback(input_channels=3, hidden_dim=16, output_dim=10)\n",
        "    dummy_input = torch.randn(1, 3, 64, 64)\n",
        "    output = net(dummy_input)\n",
        "    print(\"Recurrent Feedback Network Output Shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "-faJXffD0MKn",
        "outputId": "e19b7ae2-c8a8-4753-bfeb-b2e6cda38c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [16, 176, 3, 3], expected input[1, 16, 64, 64] to have 176 channels, but got 16 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ce7debb09c6d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetworkRecurrentFeedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recurrent Feedback Network Output Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ce7debb09c6d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mMT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMT_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Apply recurrent feedback on MT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mMT_refined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecur_MT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mVIP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVIP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMT_refined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ce7debb09c6d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Iteratively refine the representation with a residual connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 176, 3, 3], expected input[1, 16, 64, 64] to have 176 channels, but got 16 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention‑Like Gates via Spatial Maps or Channels Concept:**\n",
        "\n",
        "Attention modules learn to reweight features either spatially or across channels.\n",
        "\n",
        "**Implementation Outline:**\n",
        "\n",
        "For channel attention: Use global pooling followed by a small fully connected network to produce per‑channel weights.\n",
        "\n",
        "For spatial attention: Use a convolution to create an attention mask over spatial dimensions.\n",
        "\n",
        "Multiply the computed attention weights with the layer’s output to modulate the signal."
      ],
      "metadata": {
        "id": "jjk5fOxL3gVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Attention Gate Module ---\n",
        "class AttentionGate(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        \"\"\"\n",
        "        Creates an attention map for a feature map.\n",
        "        channels: number of input channels.\n",
        "        \"\"\"\n",
        "        super(AttentionGate, self).__init__()\n",
        "        # 1x1 convolution to compute attention weights.\n",
        "        self.attn_conv = nn.Conv2d(channels, channels, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_mask = self.sigmoid(self.attn_conv(x))\n",
        "        # Element-wise multiplication with the attention mask.\n",
        "        return x * attn_mask\n",
        "\n",
        "# --- Neural Network with Attention Gates ---\n",
        "class NeuralNetworkAttention(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_dim, output_dim, kernel_size=3, padding_size=1):\n",
        "        super(NeuralNetworkAttention, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        # Multipliers for different layers.\n",
        "        V1_p = 10; Thick_stripe_p = 1; MT_p = 1; VIP_p = 0.4; MST_p = 0.5\n",
        "        Interstripe_p = 5; Thin_stripe_p = 1; LIP_p = 1; V4_p = 4\n",
        "        PIT_p = 2.5; CIT_p = 3.5; SevenA_p = 3.5; AIT_p = 4.5\n",
        "\n",
        "        # Convolutional layers.\n",
        "        self.V1 = nn.Conv2d(input_channels, int(hidden_dim * V1_p), kernel_size, padding=padding_size)\n",
        "        self.ThickStripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Thick_stripe_p), kernel_size, padding=padding_size)\n",
        "        self.MT = nn.Conv2d(int(hidden_dim * (V1_p + Thick_stripe_p)), int(hidden_dim * MT_p), kernel_size, padding=padding_size)\n",
        "        self.VIP = nn.Conv2d(int(hidden_dim * MT_p), int(hidden_dim * VIP_p), kernel_size, padding=padding_size)\n",
        "        self.MST = nn.Conv2d(int(hidden_dim * (MT_p + VIP_p)), int(hidden_dim * MST_p), kernel_size, padding=padding_size)\n",
        "        self.Interstripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Interstripe_p), kernel_size, padding=padding_size)\n",
        "        self.ThinStripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Thin_stripe_p), kernel_size, padding=padding_size)\n",
        "        self.LIP = nn.Conv2d(int(hidden_dim * MST_p), int(hidden_dim * LIP_p), kernel_size, padding=padding_size)\n",
        "        self.V4 = nn.Conv2d(int(hidden_dim * (MT_p + Interstripe_p + Thin_stripe_p)), int(hidden_dim * V4_p), kernel_size, padding=padding_size)\n",
        "        self.PIT = nn.Conv2d(int(hidden_dim * (V4_p + MST_p + LIP_p)), int(hidden_dim * PIT_p), kernel_size, padding=padding_size)\n",
        "        self.CIT = nn.Conv2d(int(hidden_dim * (PIT_p + V4_p)), int(hidden_dim * CIT_p), kernel_size, padding=padding_size)\n",
        "        self.SevenA = nn.Conv2d(int(hidden_dim * (MST_p + LIP_p)), int(hidden_dim * SevenA_p), kernel_size, padding=padding_size)\n",
        "        self.AIT = nn.Conv2d(int(hidden_dim * (CIT_p + SevenA_p)), output_dim, kernel_size, padding=padding_size)\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Attention gates applied to selected layers.\n",
        "        self.attn_V1 = AttentionGate(int(hidden_dim * V1_p))\n",
        "        self.attn_MT = AttentionGate(int(hidden_dim * MT_p))\n",
        "        self.attn_V4 = AttentionGate(int(hidden_dim * V4_p))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process through V1 and apply attention.\n",
        "        V1 = self.relu(self.V1(x))\n",
        "        V1 = self.attn_V1(V1)\n",
        "        ThickStripe = self.relu(self.ThickStripe(V1))\n",
        "        MT_input = torch.cat((V1, ThickStripe), dim=1)\n",
        "        MT = self.relu(self.MT(MT_input))\n",
        "        MT = self.attn_MT(MT)\n",
        "        VIP = self.relu(self.VIP(MT))\n",
        "        MST_input = torch.cat((MT, VIP), dim=1)\n",
        "        MST = self.relu(self.MST(MST_input))\n",
        "        Interstripe = self.relu(self.Interstripe(V1))\n",
        "        ThinStripe = self.relu(self.ThinStripe(V1))\n",
        "        LIP = self.relu(self.LIP(MST))\n",
        "        V4_input = torch.cat((MT, Interstripe, ThinStripe), dim=1)\n",
        "        V4 = self.relu(self.V4(V4_input))\n",
        "        V4 = self.attn_V4(V4)\n",
        "        PIT_input = torch.cat((V4, MST, LIP), dim=1)\n",
        "        PIT = self.relu(self.PIT(PIT_input))\n",
        "        CIT_input = torch.cat((PIT, V4), dim=1)\n",
        "        CIT = self.relu(self.CIT(CIT_input))\n",
        "        SevenA = self.relu(self.SevenA(torch.cat((MST, LIP), dim=1)))\n",
        "        AIT_input = torch.cat((CIT, SevenA), dim=1)\n",
        "        AIT = self.AIT(AIT_input)\n",
        "\n",
        "        out = self.pool(AIT)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "\n",
        "# --- Testing the Attention Gates Network ---\n",
        "if __name__ == '__main__':\n",
        "    net = NeuralNetworkAttention(input_channels=3, hidden_dim=16, output_dim=10)\n",
        "    dummy_input = torch.randn(1, 3, 64, 64)\n",
        "    output = net(dummy_input)\n",
        "    print(\"Attention Gates Network Output Shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIEIxPGG0f3c",
        "outputId": "c362c6ad-b687-454b-9734-51e050c8a97f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Gates Network Output Shape: torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lateral Interaction via Recurrent Connections Within the Same Layer Concept:**\n",
        "\n",
        "Lateral (horizontal) interactions allow neurons within the same layer to influence each other.\n",
        "\n",
        "This mimics lateral connectivity seen in the cortex.\n",
        "\n",
        "**Implementation Outline:**\n",
        "\n",
        "Add an extra convolution that “loops” over the output of a layer.\n",
        "\n",
        "Use a recurrent formulation (or a simple residual connection) so that the lateral activity is iteratively refined."
      ],
      "metadata": {
        "id": "w7wuPh-F3nH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Lateral Interaction Module ---\n",
        "class LateralInteraction(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        \"\"\"\n",
        "        Implements lateral interactions using a convolution.\n",
        "        channels: number of input channels.\n",
        "        \"\"\"\n",
        "        super(LateralInteraction, self).__init__()\n",
        "        self.lateral_conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute lateral features and add them back to the original features.\n",
        "        lateral = F.relu(self.lateral_conv(x))\n",
        "        return x + lateral\n",
        "\n",
        "# --- Neural Network with Lateral Interactions ---\n",
        "class NeuralNetworkLateralInteraction(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_dim, output_dim, kernel_size=3, padding_size=1):\n",
        "        super(NeuralNetworkLateralInteraction, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        # Multipliers for various layers.\n",
        "        V1_p = 10; Thick_stripe_p = 1; MT_p = 1; VIP_p = 0.4; MST_p = 0.5\n",
        "        Interstripe_p = 5; Thin_stripe_p = 1; LIP_p = 1; V4_p = 4\n",
        "        PIT_p = 2.5; CIT_p = 3.5; SevenA_p = 3.5; AIT_p = 4.5\n",
        "\n",
        "        # Define the convolutional layers.\n",
        "        self.V1 = nn.Conv2d(input_channels, int(hidden_dim * V1_p), kernel_size, padding=padding_size)\n",
        "        self.ThickStripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Thick_stripe_p), kernel_size, padding=padding_size)\n",
        "        self.MT = nn.Conv2d(int(hidden_dim * (V1_p + Thick_stripe_p)), int(hidden_dim * MT_p), kernel_size, padding=padding_size)\n",
        "        self.VIP = nn.Conv2d(int(hidden_dim * MT_p), int(hidden_dim * VIP_p), kernel_size, padding=padding_size)\n",
        "        self.MST = nn.Conv2d(int(hidden_dim * (MT_p + VIP_p)), int(hidden_dim * MST_p), kernel_size, padding=padding_size)\n",
        "        self.Interstripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Interstripe_p), kernel_size, padding=padding_size)\n",
        "        self.ThinStripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Thin_stripe_p), kernel_size, padding=padding_size)\n",
        "        self.LIP = nn.Conv2d(int(hidden_dim * MST_p), int(hidden_dim * LIP_p), kernel_size, padding=padding_size)\n",
        "        self.V4 = nn.Conv2d(int(hidden_dim * (MT_p + Interstripe_p + Thin_stripe_p)), int(hidden_dim * V4_p), kernel_size, padding=padding_size)\n",
        "        self.PIT = nn.Conv2d(int(hidden_dim * (V4_p + MST_p + LIP_p)), int(hidden_dim * PIT_p), kernel_size, padding=padding_size)\n",
        "        self.CIT = nn.Conv2d(int(hidden_dim * (PIT_p + V4_p)), int(hidden_dim * CIT_p), kernel_size, padding=padding_size)\n",
        "        self.SevenA = nn.Conv2d(int(hidden_dim * (MST_p + LIP_p)), int(hidden_dim * SevenA_p), kernel_size, padding=padding_size)\n",
        "        self.AIT = nn.Conv2d(int(hidden_dim * (CIT_p + SevenA_p)), output_dim, kernel_size, padding=padding_size)\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Lateral interaction applied to V1.\n",
        "        self.lateral_V1 = LateralInteraction(int(hidden_dim * V1_p))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process V1 and apply lateral interactions.\n",
        "        V1 = self.relu(self.V1(x))\n",
        "        V1 = self.lateral_V1(V1)\n",
        "        ThickStripe = self.relu(self.ThickStripe(V1))\n",
        "        MT_input = torch.cat((V1, ThickStripe), dim=1)\n",
        "        MT = self.relu(self.MT(MT_input))\n",
        "        VIP = self.relu(self.VIP(MT))\n",
        "        MST_input = torch.cat((MT, VIP), dim=1)\n",
        "        MST = self.relu(self.MST(MST_input))\n",
        "        Interstripe = self.relu(self.Interstripe(V1))\n",
        "        ThinStripe = self.relu(self.ThinStripe(V1))\n",
        "        LIP = self.relu(self.LIP(MST))\n",
        "        V4_input = torch.cat((MT, Interstripe, ThinStripe), dim=1)\n",
        "        V4 = self.relu(self.V4(V4_input))\n",
        "        PIT_input = torch.cat((V4, MST, LIP), dim=1)\n",
        "        PIT = self.relu(self.PIT(PIT_input))\n",
        "        CIT_input = torch.cat((PIT, V4), dim=1)\n",
        "        CIT = self.relu(self.CIT(CIT_input))\n",
        "        SevenA = self.relu(self.SevenA(torch.cat((MST, LIP), dim=1)))\n",
        "        AIT_input = torch.cat((CIT, SevenA), dim=1)\n",
        "        AIT = self.AIT(AIT_input)\n",
        "\n",
        "        out = self.pool(AIT)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "\n",
        "# --- Testing the Lateral Interaction Network ---\n",
        "if __name__ == '__main__':\n",
        "    net = NeuralNetworkLateralInteraction(input_channels=3, hidden_dim=16, output_dim=10)\n",
        "    dummy_input = torch.randn(1, 3, 64, 64)\n",
        "    output = net(dummy_input)\n",
        "    print(\"Lateral Interaction Network Output Shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hkvJCik0kfo",
        "outputId": "41d09b1e-06d0-48a4-bbb3-c2ded17cc0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lateral Interaction Network Output Shape: torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A concept of combining all previous modules**"
      ],
      "metadata": {
        "id": "LDifH8Cj3va2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------------\n",
        "# Auxiliary Modules\n",
        "# -------------------------------\n",
        "\n",
        "# 1. Attention Gate Module\n",
        "class AttentionGate(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements an attention mechanism that computes a spatially or channel-wise gating mask.\n",
        "    \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(AttentionGate, self).__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn = self.sigmoid(self.conv(x))\n",
        "        return x * attn\n",
        "\n",
        "# 2. Recurrent Block for Feedback Loops\n",
        "class RecurrentBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Wraps a convolutional layer into a recurrent formulation with residual connections.\n",
        "    Iteratively refines the feature representation.\n",
        "    \"\"\"\n",
        "    def __init__(self, conv_layer, num_iterations=3):\n",
        "        super(RecurrentBlock, self).__init__()\n",
        "        self.conv_layer = conv_layer\n",
        "        self.num_iterations = num_iterations\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        for _ in range(self.num_iterations):\n",
        "            h = F.relu(self.conv_layer(h) + x)\n",
        "        return h\n",
        "\n",
        "# 3. Predictive Coding Block\n",
        "class PredictiveCodingBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a simplified predictive coding scheme where a top-down pathway generates\n",
        "    a prediction for a lower-level feature map and the error (difference) is computed.\n",
        "    \"\"\"\n",
        "    def __init__(self, bottom_up_conv, top_down_conv):\n",
        "        super(PredictiveCodingBlock, self).__init__()\n",
        "        self.bottom_up_conv = bottom_up_conv  # e.g., V1 convolution\n",
        "        self.top_down_conv = top_down_conv    # prediction from higher layer (e.g., AIT)\n",
        "\n",
        "    def forward(self, lower_input, higher_prediction):\n",
        "        bottom_up = self.bottom_up_conv(lower_input)\n",
        "        top_down = self.top_down_conv(higher_prediction)\n",
        "        error = F.relu(bottom_up - top_down)\n",
        "        return error\n",
        "\n",
        "# 4. Lateral Interaction Module\n",
        "class LateralInteraction(nn.Module):\n",
        "    \"\"\"\n",
        "    Simulates horizontal interactions within the same cortical area (e.g., V1)\n",
        "    using a convolution followed by a residual addition.\n",
        "    \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(LateralInteraction, self).__init__()\n",
        "        self.lateral_conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lateral = F.relu(self.lateral_conv(x))\n",
        "        return x + lateral\n",
        "\n",
        "# 5. Multi-Scale Integration Block\n",
        "class MultiScaleBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Captures features at multiple scales by applying parallel convolutions with different kernel sizes.\n",
        "    The outputs are concatenated to form an enriched representation.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(MultiScaleBlock, self).__init__()\n",
        "        self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=2)\n",
        "        self.conv7 = nn.Conv2d(in_channels, out_channels, kernel_size=7, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out3 = F.relu(self.conv3(x))\n",
        "        out5 = F.relu(self.conv5(x))\n",
        "        out7 = F.relu(self.conv7(x))\n",
        "        return torch.cat((out3, out5, out7), dim=1)\n",
        "\n",
        "# 6. Neuromodulation Module\n",
        "class Neuromodulation(nn.Module):\n",
        "    \"\"\"\n",
        "    Applies a learnable multiplicative modulation to the activations.\n",
        "    This can mimic global modulatory signals (e.g., dopamine) in the cortex.\n",
        "    \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(Neuromodulation, self).__init__()\n",
        "        self.modulation = nn.Parameter(torch.ones(1, channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.modulation\n",
        "\n",
        "# -------------------------------\n",
        "# Visual Cortex-Inspired Network\n",
        "# -------------------------------\n",
        "\n",
        "class VisualCortexNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network model inspired by the architecture of the visual cortex.\n",
        "    Combines bottom-up processing with top-down predictive coding, recurrent feedback,\n",
        "    attention gating, lateral interactions, multi-scale feature integration,\n",
        "    sparse coding (via dropout), and neuromodulation.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_dim, output_dim, kernel_size=3, padding_size=1, dropout_prob=0.2):\n",
        "        super(VisualCortexNetwork, self).__init__()\n",
        "        # Define multipliers for different brain areas\n",
        "        V1_p = 10\n",
        "        Thick_stripe_p = 1\n",
        "        MT_p = 1\n",
        "        VIP_p = 0.4\n",
        "        MST_p = 0.5\n",
        "        Interstripe_p = 5\n",
        "        Thin_stripe_p = 1\n",
        "        LIP_p = 1\n",
        "        V4_p = 4\n",
        "        PIT_p = 2.5\n",
        "        CIT_p = 3.5\n",
        "        SevenA_p = 3.5\n",
        "        AIT_p = 4.5\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout2d(p=dropout_prob)\n",
        "\n",
        "        # ----- V1 Stage with Multi-Scale Integration, Lateral, Attention, and Neuromodulation -----\n",
        "        # Traditional V1 convolution path\n",
        "        self.V1 = nn.Conv2d(input_channels, int(hidden_dim * V1_p), kernel_size=kernel_size, padding=padding_size)\n",
        "        # Multi-scale integration on the input (for richer early representations)\n",
        "        # We split the output channels equally among the three scales\n",
        "        ms_out_channels = int((hidden_dim * V1_p) // 3)\n",
        "        self.V1_ms = MultiScaleBlock(input_channels, ms_out_channels)\n",
        "        # Lateral interactions within V1\n",
        "        self.lateral_V1 = LateralInteraction(int(hidden_dim * V1_p))\n",
        "        # Attention gate on V1\n",
        "        self.attn_V1 = AttentionGate(int(hidden_dim * V1_p))\n",
        "        # Neuromodulatory gain control in V1\n",
        "        self.neuro_V1 = Neuromodulation(int(hidden_dim * V1_p))\n",
        "\n",
        "        # ----- Thick Stripe (receives V1 output) -----\n",
        "        self.ThickStripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Thick_stripe_p), kernel_size=kernel_size, padding=padding_size)\n",
        "\n",
        "        # ----- MT Stage: combining V1 and ThickStripe -----\n",
        "        self.MT = nn.Conv2d(int(hidden_dim * (V1_p + Thick_stripe_p)), int(hidden_dim * MT_p), kernel_size=kernel_size, padding=padding_size)\n",
        "        self.recur_MT = RecurrentBlock(self.MT, num_iterations=3)\n",
        "        self.attn_MT = AttentionGate(int(hidden_dim * MT_p))\n",
        "        self.neuro_MT = Neuromodulation(int(hidden_dim * MT_p))\n",
        "\n",
        "        # ----- VIP Stage: receives MT output -----\n",
        "        self.VIP = nn.Conv2d(int(hidden_dim * MT_p), int(hidden_dim * VIP_p), kernel_size=kernel_size, padding=padding_size)\n",
        "\n",
        "        # ----- MST Stage: combining MT and VIP -----\n",
        "        self.MST = nn.Conv2d(int(hidden_dim * (MT_p + VIP_p)), int(hidden_dim * MST_p), kernel_size=kernel_size, padding=padding_size)\n",
        "        self.recur_MST = RecurrentBlock(self.MST, num_iterations=3)\n",
        "\n",
        "        # ----- Interstripe and ThinStripe: parallel streams from V1 -----\n",
        "        self.Interstripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Interstripe_p), kernel_size=kernel_size, padding=padding_size)\n",
        "        self.ThinStripe = nn.Conv2d(int(hidden_dim * V1_p), int(hidden_dim * Thin_stripe_p), kernel_size=kernel_size, padding=padding_size)\n",
        "\n",
        "        # ----- LIP Stage: receives MST output -----\n",
        "        self.LIP = nn.Conv2d(int(hidden_dim * MST_p), int(hidden_dim * LIP_p), kernel_size=kernel_size, padding=padding_size)\n",
        "\n",
        "        # ----- V4 Stage: integrates MT, Interstripe, and ThinStripe -----\n",
        "        self.V4 = nn.Conv2d(int(hidden_dim * (MT_p + Interstripe_p + Thin_stripe_p)), int(hidden_dim * V4_p), kernel_size=kernel_size, padding=padding_size)\n",
        "        self.attn_V4 = AttentionGate(int(hidden_dim * V4_p))\n",
        "        self.neuro_V4 = Neuromodulation(int(hidden_dim * V4_p))\n",
        "\n",
        "        # ----- PIT Stage: integrates V4, MST, and LIP -----\n",
        "        self.PIT = nn.Conv2d(int(hidden_dim * (V4_p + MST_p + LIP_p)), int(hidden_dim * PIT_p), kernel_size=kernel_size, padding=padding_size)\n",
        "\n",
        "        # ----- CIT Stage: integrates PIT and V4 -----\n",
        "        self.CIT = nn.Conv2d(int(hidden_dim * (PIT_p + V4_p)), int(hidden_dim * CIT_p), kernel_size=kernel_size, padding=padding_size)\n",
        "\n",
        "        # ----- SevenA Stage: integrates MST and LIP -----\n",
        "        self.SevenA = nn.Conv2d(int(hidden_dim * (MST_p + LIP_p)), int(hidden_dim * SevenA_p), kernel_size=kernel_size, padding=padding_size)\n",
        "\n",
        "        # ----- AIT Stage: integrates CIT and SevenA -----\n",
        "        self.AIT = nn.Conv2d(int(hidden_dim * (CIT_p + SevenA_p)), int(hidden_dim * AIT_p), kernel_size=kernel_size, padding=padding_size)\n",
        "\n",
        "        # ----- Predictive Coding Feedback from AIT to V1 -----\n",
        "        self.pred_V1 = PredictiveCodingBlock(\n",
        "            self.V1,\n",
        "            nn.Conv2d(int(hidden_dim * AIT_p), int(hidden_dim * V1_p), kernel_size=kernel_size, padding=padding_size)\n",
        "        )\n",
        "\n",
        "        # ----- Final Classification -----\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(int(hidden_dim * AIT_p), output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # --- V1 Stage ---\n",
        "        # Traditional V1 pathway\n",
        "        v1_trad = self.relu(self.V1(x))\n",
        "        # Multi-scale pathway on the raw input\n",
        "        v1_ms = self.V1_ms(x)\n",
        "        # To combine the two paths, adjust channels if needed.\n",
        "        # Here we assume that v1_trad and v1_ms have the same spatial dimensions.\n",
        "        # A simple sum (or concatenation followed by a conv) is used.\n",
        "        # For demonstration, we add the multi-scale output (after projecting channels)\n",
        "        # Note: if dimensions differ, an extra 1x1 conv may be required.\n",
        "        V1 = self.relu(v1_trad + v1_ms)\n",
        "        # Apply lateral interactions, attention, neuromodulation, and dropout for sparsity.\n",
        "        V1 = self.lateral_V1(V1)\n",
        "        V1 = self.attn_V1(V1)\n",
        "        V1 = self.neuro_V1(V1)\n",
        "        V1 = self.dropout(V1)\n",
        "\n",
        "        # --- ThickStripe Stage ---\n",
        "        ThickStripe = self.relu(self.ThickStripe(V1))\n",
        "        ThickStripe = self.dropout(ThickStripe)\n",
        "\n",
        "        # --- MT Stage ---\n",
        "        MT_input = torch.cat((V1, ThickStripe), dim=1)\n",
        "        MT = self.relu(self.MT(MT_input))\n",
        "        MT = self.recur_MT(MT)\n",
        "        MT = self.attn_MT(MT)\n",
        "        MT = self.neuro_MT(MT)\n",
        "        MT = self.dropout(MT)\n",
        "\n",
        "        # --- VIP Stage ---\n",
        "        VIP = self.relu(self.VIP(MT))\n",
        "        VIP = self.dropout(VIP)\n",
        "\n",
        "        # --- MST Stage ---\n",
        "        MST_input = torch.cat((MT, VIP), dim=1)\n",
        "        MST = self.relu(self.MST(MST_input))\n",
        "        MST = self.recur_MST(MST)\n",
        "        MST = self.dropout(MST)\n",
        "\n",
        "        # --- Interstripe and ThinStripe Stages ---\n",
        "        Interstripe = self.relu(self.Interstripe(V1))\n",
        "        ThinStripe = self.relu(self.ThinStripe(V1))\n",
        "        Interstripe = self.dropout(Interstripe)\n",
        "        ThinStripe = self.dropout(ThinStripe)\n",
        "\n",
        "        # --- LIP Stage ---\n",
        "        LIP = self.relu(self.LIP(MST))\n",
        "        LIP = self.dropout(LIP)\n",
        "\n",
        "        # --- V4 Stage ---\n",
        "        V4_input = torch.cat((MT, Interstripe, ThinStripe), dim=1)\n",
        "        V4 = self.relu(self.V4(V4_input))\n",
        "        V4 = self.attn_V4(V4)\n",
        "        V4 = self.neuro_V4(V4)\n",
        "        V4 = self.dropout(V4)\n",
        "\n",
        "        # --- PIT Stage ---\n",
        "        PIT_input = torch.cat((V4, MST, LIP), dim=1)\n",
        "        PIT = self.relu(self.PIT(PIT_input))\n",
        "        PIT = self.dropout(PIT)\n",
        "\n",
        "        # --- CIT Stage ---\n",
        "        CIT_input = torch.cat((PIT, V4), dim=1)\n",
        "        CIT = self.relu(self.CIT(CIT_input))\n",
        "        CIT = self.dropout(CIT)\n",
        "\n",
        "        # --- SevenA Stage ---\n",
        "        SevenA = self.relu(self.SevenA(torch.cat((MST, LIP), dim=1)))\n",
        "        SevenA = self.dropout(SevenA)\n",
        "\n",
        "        # --- AIT Stage ---\n",
        "        AIT_input = torch.cat((CIT, SevenA), dim=1)\n",
        "        AIT = self.relu(self.AIT(AIT_input))\n",
        "        AIT = self.dropout(AIT)\n",
        "\n",
        "        # --- Predictive Coding Feedback ---\n",
        "        # Use AIT to predict V1 features and compute an error signal.\n",
        "        pred_error = self.pred_V1(x, AIT)\n",
        "        V1_updated = V1 + pred_error  # This updated V1 could be used in iterative schemes.\n",
        "\n",
        "        # --- Final Classification ---\n",
        "        pooled = self.pool(AIT)\n",
        "        pooled = pooled.view(pooled.size(0), -1)\n",
        "        out = self.fc(pooled)\n",
        "        return out\n",
        "\n",
        "# -------------------------------\n",
        "# Testing the Network\n",
        "# -------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Create an instance of the network.\n",
        "    # For example, using 3 input channels (RGB), a hidden dimension of 16, and 10 output classes.\n",
        "    model = VisualCortexNetwork(input_channels=3, hidden_dim=16, output_dim=10)\n",
        "\n",
        "    # Create a dummy input tensor (batch size 8, 3 channels, 64x64 spatial dimensions)\n",
        "    x = torch.randn(8, 3, 64, 64)\n",
        "\n",
        "    # Forward pass through the network\n",
        "    out = model(x)\n",
        "\n",
        "    # Print the output shape (should be [8, 10])\n",
        "    print(\"Output shape:\", out.shape)"
      ],
      "metadata": {
        "id": "G4cG5Jqj0wNe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}